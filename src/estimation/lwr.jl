# ------------------------------------------------------------------
# Licensed under the MIT License. See LICENCE in the project root.
# ------------------------------------------------------------------

"""
    LWR(varâ‚=>paramâ‚, varâ‚‚=>paramâ‚‚, ...)

Locally weighted regression estimation solver.

## Parameters

* `neighbors` - Number of neighbors (default to all the data)
* `distance`  - A distance from Distances.jl (default to `Euclidean()`)
* `weightfun` - Weighting function (default to `exp(-3*h^2/2)`)

### References

* Stone 1977. *Consistent non-parametric regression.*
* Cleveland 1979. *Robust locally weighted regression and smoothing scatterplots.*
* Cleveland & Grosse 1991. *Computational methods for local regression.*
"""
@estimsolver LWR begin
  @param neighbors = nothing
  @param distance = Euclidean()
  @param weightfun = h -> exp(-3*h^2)
end

function solve(problem::EstimationProblem, solver::LWR)
  # retrieve problem info
  pdata = data(problem)
  dtable = values(pdata)
  pdomain = domain(problem)

  mactypeof = Dict(name(v) => mactype(v) for v in variables(problem))

  # result for each variable
  Î¼s = []; Ïƒs = []

  for covars in covariables(problem, solver)
    for var in covars.names
      # get user parameters
      varparams = covars.params[(var,)]

      # determine value type
      V = mactypeof[var]

      # retrieve non-missing data
      dcols = Tables.columns(dtable)
      dvals = Tables.getcolumn(dcols, var)
      dinds = findall(!ismissing, dvals)
      ğ’® = view(pdata, dinds)
      ğ’Ÿ = domain(ğ’®)
      ğ’¯ = values(ğ’®)
      n = nelements(ğ’Ÿ)

      # determine number of nearest neighbors to use
      k = isnothing(varparams.neighbors) ? n : varparams.neighbors

      # determine distance type
      D = varparams.distance

      # weight function
      w = varparams.weightfun

      @assert n > 0 "estimation requires data"

      @assert k â‰¤ n "invalid number of neighbors"

      # fit search tree
      X = [coordinates(centroid(ğ’Ÿ, i)) for i in 1:n]
      if D isa NearestNeighbors.MinkowskiMetric
        tree = KDTree(X, D)
      else
        tree = BallTree(X, D)
      end

      # adjust unit
      cols = Tables.columns(ğ’¯)
      vals = Tables.getcolumn(cols, var)
      unit = elunit(vals)
      z = uadjust(unit, vals)

      # estimation loop
      inds = traverse(pdomain, LinearPath())
      pred = map(inds) do ind
        x = coordinates(centroid(pdomain, ind))

        # find neighbors
        is, ds = knn(tree, x, k)
        Î´s = ds ./ maximum(ds)

        # weighted least-squares
        Wâ‚— = Diagonal(w.(Î´s))
        Xâ‚— = [ones(eltype(x), k) reduce(hcat, X[is])']
        zâ‚— = view(z, is)
        Î¸â‚— = Xâ‚—'*Wâ‚—*Xâ‚— \ Xâ‚—'*Wâ‚—*zâ‚—

        # linear combination of response values
        xâ‚’ = [one(eltype(x)); x]
        zÌ‚â‚’ = Î¸â‚— â‹… xâ‚’
        râ‚— = Wâ‚—*Xâ‚—*(Xâ‚—'*Wâ‚—*Xâ‚—\xâ‚’)
        rÌ‚â‚’ = norm(râ‚—)

        zÌ‚â‚’, rÌ‚â‚’
      end

      varÎ¼ = first.(pred)
      varÏƒ = last.(pred)

      push!(Î¼s, var => urevert(unit, varÎ¼))
      push!(Ïƒs, Symbol(var, "_variance") => varÏƒ * absoluteunit(unit)^2)
    end
  end

  georef((; Î¼s..., Ïƒs...), pdomain)
end
